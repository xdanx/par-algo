After we made sure that the parallel implementation of the algorithm produces results that
are accurate enough, we proceeded to the next step.
A parallel version of the \texttt{EULER} algorithm was implemented using the \emph{Message Passing Interface} (\texttt{MPI}).

In order to use the version of \texttt{MPI} installed on the labs machine, we have firstly created a host files,
specifying  the machines on which the program will run.

The in order, the \texttt{EULER} algorithm was parallelised in the following way:
\begin{enumerate}
 \item{Initialise the \texttt{MPI} execution environment by calling the \texttt{MPI\_Init} function.}
 \item{Call \texttt{MPI\_Comm\_rank} in order to determines the rank of the calling process in the communication system.}
 \item{Call \texttt{MPI\_Comm\_size}, in order to determine how many processors are running the program.}
\end{enumerate}

Then, the root will generate the array of points \texttt{Times} where the function $f$ will be evaluated.
After the generation step is done, the root will pass the number of generated points (the actual size of 
\texttt{Times} vector) to the rest of the processors.
\newline

In order to do a \texttt{Scatter}, each processor should know the number of points it evaluates. These
values are stored in the \texttt{sendcnts} array. If the size of \texttt{Times} will not evenly divide to the
number of processors, the overflow will be distributed to the $sencnts$.

At this stage, the displacement (from which to take the outgoing data to process) is also computed: 
this is simple since we know how many points are allocated per processor.


Finally, the \texttt{root} will create an array to receive the result of \texttt{MPI\_Gatherv} in the array $f_{ts}$.
The array will be printed to the standard output.

As a remark, for each expensive call, the method \texttt{CHECK} was used in order to verify the result of an \texttt{MPI} function call. 
If the result is not \texttt{MPI\_SUCCESS}, the program will end. The function was applied on the calls to \texttt{MPI\_Bcast}, \texttt{MPI\_Scatterv}  and \texttt{MPI\_Gatherv}.

By using \texttt{MPI} we have made the execution time of computing \texttt{f(t)} decrease (the obvious advantage of using parallelism). 
However the inter-processor communication times have increased, but the total time was significantly improved, compared
to the serial implementation. This enhancement can be best observed when the size of the vector of argument points for $f$ is large and the algorithm executes an

